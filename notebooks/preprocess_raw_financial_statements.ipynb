{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e189eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/01 18:14:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"preprocess_raw_financial_statements\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3fd0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2020_Q1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2020_Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2020_Q3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2020_Q4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "TEST_MODE = True\n",
    "RAW_DATA_DIR = \"/workspace/data/raw/\"\n",
    "\n",
    "union_df = None\n",
    "\n",
    "for dirname in os.listdir(RAW_DATA_DIR):\n",
    "    if TEST_MODE and not dirname.startswith(\"2020\"):\n",
    "        continue\n",
    "\n",
    "    print(f\" - {dirname}\")\n",
    "\n",
    "    num_df = spark.read.csv(\n",
    "        path=os.path.join(RAW_DATA_DIR, dirname, \"num.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    ) \\\n",
    "        .filter(col(\"version\").startswith(\"us-gaap\")) \\\n",
    "        .filter(col(\"value\").isNotNull()) \\\n",
    "        .filter(col(\"segments\").isNull()) \\\n",
    "        .filter(col(\"coreg\").isNull()) \\\n",
    "        .filter(col(\"uom\") == \"USD\") \\\n",
    "        .filter(col(\"qtrs\").isin([0, 4])) \\\n",
    "        .select(\"adsh\", \"tag\", \"ddate\", \"value\")\n",
    "\n",
    "    sub_df = spark.read.csv(\n",
    "        path=os.path.join(RAW_DATA_DIR, dirname, \"sub.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    ) \\\n",
    "        .filter(col(\"prevrpt\") == 0) \\\n",
    "        .filter(col(\"form\") == \"10-K\") \\\n",
    "        .select(\"adsh\", \"cik\", \"sic\", \"filed\")\n",
    "\n",
    "    joined_df = num_df.join(sub_df, on=\"adsh\", how=\"inner\").drop(\"adsh\")\n",
    "\n",
    "    union_df = joined_df if union_df is None else union_df.union(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "678e1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "window = Window.partitionBy(\"cik\", \"tag\", \"ddate\").orderBy(desc(\"filed\"))\n",
    "deduped_df = union_df.withColumn(\"row_num\", row_number().over(window)) \\\n",
    "               .filter(\"row_num = 1\").drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343aa348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def industry_mapping_func(industries_filepath: str):\n",
    "    industries_map = {}\n",
    "\n",
    "    with open(industries_filepath, \"r\") as industries_file:\n",
    "        current_industry = None\n",
    "\n",
    "        for line in industries_file:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.endswith(\":\"):\n",
    "                current_industry = line[:-1]\n",
    "                industries_map[current_industry] = []\n",
    "            elif len(line) > 0:\n",
    "                min_sic, max_sic = map(int, line.split(\"-\"))\n",
    "                industries_map[current_industry].append((min_sic, max_sic))\n",
    "\n",
    "    def map_sic_to_industry(sic: int) -> str:\n",
    "        for industry, ranges in industries_map.items():\n",
    "            for min_sic, max_sic in ranges:\n",
    "                if min_sic <= sic <= max_sic:\n",
    "                    return industry\n",
    "        return \"Other\"\n",
    "\n",
    "    return map_sic_to_industry\n",
    "\n",
    "sic_mapper_udf = udf(\n",
    "    industry_mapping_func(\"/workspace/data/fama-french-industries.txt\"),\n",
    "    StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d788aec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===========================================>          (90 + 22) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----+----+----------+\n",
      "|                 tag|     industry| cik|year|     value|\n",
      "+--------------------+-------------+----+----+----------+\n",
      "|AssetImpairmentCh...|Manufacturing|1750|2020| 8100000.0|\n",
      "|CashAndCashEquiva...|Manufacturing|1750|2020|   4.047E8|\n",
      "|CashProvidedByUse...|Manufacturing|1750|2019| -500000.0|\n",
      "|    CostsAndExpenses|Manufacturing|1750|2018|  1.6628E9|\n",
      "|EffectOfExchangeR...|Manufacturing|1750|2018| -100000.0|\n",
      "|            Goodwill|Manufacturing|1750|2018|   1.187E8|\n",
      "|IncreaseDecreaseI...|Manufacturing|1750|2019|    3.44E7|\n",
      "|IncreaseDecreaseI...|Manufacturing|1750|2018|-3400000.0|\n",
      "|InvestmentIncomeI...|Manufacturing|1750|2018|  100000.0|\n",
      "|PaymentsForProcee...|Manufacturing|1750|2020| 2800000.0|\n",
      "|AccountsPayableTr...|   Healthcare|1800|2019|   3.252E9|\n",
      "|AmortizationOfFin...|   Healthcare|1800|2017| 5000000.0|\n",
      "|CashAndCashEquiva...|   Healthcare|1800|2019|    3.86E9|\n",
      "|ConstructionInPro...|   Healthcare|1800|2018|    8.94E8|\n",
      "|ConstructionInPro...|   Healthcare|1800|2019|    1.11E9|\n",
      "|IncreaseDecreaseI...|   Healthcare|1800|2017|    2.07E8|\n",
      "|InvestmentIncomeI...|   Healthcare|1800|2017|    1.24E8|\n",
      "|OtherComprehensiv...|   Healthcare|1800|2018|    -4.7E7|\n",
      "|ProceedsFromIssua...|   Healthcare|1800|2018|   4.009E9|\n",
      "| ShortTermBorrowings|   Healthcare|1800|2018|     2.0E8|\n",
      "+--------------------+-------------+----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "final_df = deduped_df.withColumn(\"year\", substring(\"ddate\", 1, 4)) \\\n",
    "               .withColumn(\"industry\", sic_mapper_udf(\"sic\")) \\\n",
    "               .select(\"tag\", \"industry\", \"cik\", \"year\", \"value\")\n",
    "\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d9d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:============================================>         (92 + 20) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------------+------+\n",
      "|year|            industry|           value|metric|\n",
      "+----+--------------------+----------------+------+\n",
      "|2020|   Consumer Durables|    9.64858687E8|profit|\n",
      "|2018|  Telecommunications| 3.8866045969E10|profit|\n",
      "|2010|          Technology|         1.969E7|profit|\n",
      "|2011|          Technology|       -278000.0|profit|\n",
      "|2019|       Manufacturing| 5.3254011787E10|profit|\n",
      "|2018|               Other|1.28920399787E11|profit|\n",
      "|2020|Consumer Nondurables|    7.70115507E9|profit|\n",
      "|2019|          Healthcare| 5.5588299566E10|profit|\n",
      "|2020|Retail, Wholesale...| 4.1647079114E10|profit|\n",
      "|2017|Consumer Nondurables|  4.692097116E10|profit|\n",
      "|2016|           Chemicals|          6.48E8|profit|\n",
      "|2018|           Utilities| 4.9517532738E10|profit|\n",
      "|2017|  Telecommunications|    7.1878395E10|profit|\n",
      "|2019|           Chemicals| 1.6472051893E10|profit|\n",
      "|2020|           Chemicals|   2.996184334E9|profit|\n",
      "|2017|       Manufacturing| 3.1188103871E10|profit|\n",
      "|2020|  Telecommunications|    2.16609666E8|profit|\n",
      "|2018|Retail, Wholesale...|1.04161646461E11|profit|\n",
      "|2017|          Healthcare|  -2.546672481E9|profit|\n",
      "|2020|          Healthcare|   7.170225294E9|profit|\n",
      "+----+--------------------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# for each year/industry, sum up NetIncomeLoss\n",
    "from pyspark.sql.functions import lit, sum\n",
    "\n",
    "profit_df = final_df.filter(col(\"tag\") == \"NetIncomeLoss\") \\\n",
    "                .groupBy(\"year\", \"industry\") \\\n",
    "                .agg(sum(\"value\").alias(\"value\")) \\\n",
    "                .withColumn(\"metric\", lit(\"profit\"))\n",
    "\n",
    "profit_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
